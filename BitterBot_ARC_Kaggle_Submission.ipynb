{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† BitterBot AI - ARC Prize 2025 Submission\n",
    "\n",
    "**BitterBot Consciousness Architecture for Abstract Reasoning**\n",
    "\n",
    "This notebook:\n",
    "- ‚úÖ Loads pre-trained checkpoint (allowed per ARC Prize 2025 rules)\n",
    "- ‚úÖ Runs full AlphaEvolve pipeline with near-miss repair\n",
    "- ‚úÖ Outputs `submission.json` in correct format\n",
    "- ‚úÖ Completes within 12-hour Kaggle limit\n",
    "\n",
    "**Key Components:**\n",
    "- TOPAS-ARC-60M unified model\n",
    "- RelationalMemoryNeuro with Hebbian learning\n",
    "- DreamEngine with FSHO/CIO\n",
    "- AlphaEvolve orchestrator\n",
    "- Tiered near-miss repair system\n",
    "- Best-of-2 selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Seed for reproducibility\n",
    "def seed_all(seed=1338):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_all(1338)\n",
    "\n",
    "# Verify GPU\n",
    "assert torch.cuda.is_available(), '‚ùå CUDA required'\n",
    "device = torch.device('cuda')\n",
    "print(f'‚úÖ GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "# Assumes ARC-AGI-2 dataset uploaded to Kaggle\n",
    "ARC_EVAL_PATH = '/kaggle/input/arc-agi-2/arc_2_dataset/evaluation'\n",
    "\n",
    "# Assumes BitterBot code uploaded as Kaggle dataset\n",
    "BITTERBOT_PATH = '/kaggle/input/bitterbot-code'\n",
    "sys.path.insert(0, BITTERBOT_PATH)\n",
    "\n",
    "# Checkpoint path (uploaded as Kaggle dataset)\n",
    "CHECKPOINT_PATH = '/kaggle/input/bitterbot-checkpoint/alpha_arc_bundle.pt'\n",
    "\n",
    "print(f'‚úÖ Evaluation data: {ARC_EVAL_PATH}')\n",
    "print(f'‚úÖ Code path: {BITTERBOT_PATH}')\n",
    "print(f'‚úÖ Checkpoint: {CHECKPOINT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BitterBot modules\n",
    "from models.topas_arc_60M import TopasARC60M, ModelConfig\n",
    "from arc2_dataset_loader import ARC2Dataset\n",
    "from trainers.alpha_evolve import AlphaEvolver, AlphaEvolveConfig\n",
    "from models.dsl_registry import DSL_OPS\n",
    "from trainers.near_miss import near_miss_repair\n",
    "from models.topas_arc_60M import _DSLShim\n",
    "\n",
    "print('‚úÖ All BitterBot modules imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Checkpoint\n",
    "print('Loading TOPAS model...')\n",
    "\n",
    "cfg = ModelConfig(\n",
    "    width=512,\n",
    "    depth=8,\n",
    "    slots=64,\n",
    "    slot_dim=256,\n",
    "    rt_layers=10,\n",
    "    rt_heads=8,\n",
    "    max_dsl_depth=6,\n",
    "    max_beam_width=12,\n",
    "    use_ebr=True,\n",
    "    ebr_steps=5,\n",
    "    enable_dream=True,\n",
    "    enable_relmem=True,\n",
    "    pretraining_mode=True\n",
    ")\n",
    "\n",
    "model = TopasARC60M(config=cfg)\n",
    "model.to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "bundle = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "\n",
    "if isinstance(bundle, dict) and 'topas' in bundle:\n",
    "    model.load_state_dict(bundle['topas'], strict=False)\n",
    "    print('‚úÖ Loaded from bundle format')\n",
    "    \n",
    "    # Load policy/value nets if available\n",
    "    policy_net = None\n",
    "    value_net = None\n",
    "    if 'policy_head' in bundle:\n",
    "        from models.policy_nets import OpPolicyNet\n",
    "        policy_net = OpPolicyNet(input_dim=1024, hidden_dim=512).to(device)\n",
    "        policy_net.load_state_dict(bundle['policy_head'], strict=False)\n",
    "        policy_net.eval()\n",
    "        print('‚úÖ Loaded policy_net')\n",
    "    \n",
    "    if 'value_head' in bundle:\n",
    "        from models.value_net import ValueNet\n",
    "        value_net = ValueNet(context_dim=1024, program_dim=128).to(device)\n",
    "        value_net.load_state_dict(bundle['value_head'], strict=False)\n",
    "        value_net.eval()\n",
    "        print('‚úÖ Loaded value_net')\n",
    "else:\n",
    "    model.load_state_dict(bundle, strict=False)\n",
    "    print('‚úÖ Loaded from state_dict')\n",
    "    policy_net = None\n",
    "    value_net = None\n",
    "\n",
    "# Fix RelMem after checkpoint load\n",
    "if hasattr(model, '_sync_relmem_to_device'):\n",
    "    model._sync_relmem_to_device()\n",
    "\n",
    "if hasattr(model, 'relmem') and hasattr(model.relmem, 'ensure_concept_param'):\n",
    "    model.relmem.ensure_concept_param()\n",
    "\n",
    "# Reset forward counter\n",
    "if hasattr(model, '_forward_call_count'):\n",
    "    model._forward_call_count = {}\n",
    "\n",
    "model.eval()\n",
    "model.set_pretraining_mode(True)\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f'‚úÖ Model ready: {param_count:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Evaluation Dataset\n",
    "print('Loading ARC-2 evaluation dataset...')\n",
    "dataset = ARC2Dataset(challenge_file=ARC_EVAL_PATH, solution_file=None, device=device)\n",
    "print(f'‚úÖ Loaded {len(dataset)} evaluation tasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with AlphaEvolve + Near-Miss Repair\n",
    "print('='*70)\n",
    "print('Starting Evaluation with Full BitterBot Pipeline')\n",
    "print('='*70)\n",
    "\n",
    "submission = {}\n",
    "\n",
    "# Configure AlphaEvolve\n",
    "alpha_cfg = AlphaEvolveConfig(\n",
    "    use_orbit_canon=True,\n",
    "    certificates='hard',\n",
    "    use_ebr=True,\n",
    "    ebr_iters=5,\n",
    "    puct_depth=4,\n",
    "    puct_sims=500,\n",
    "    puct_c=1.5,\n",
    "    puct_beam=12,\n",
    "    enable_market=True,\n",
    "    market_liquidity=25.0,\n",
    "    hyla_max_depth=4,\n",
    "    hyla_beam_width=12,\n",
    "    self_play_enable=False,  # Disable for speed\n",
    "    alpha_dsl_enable=False   # Disable for speed\n",
    ")\n",
    "\n",
    "evolver = AlphaEvolver(model, dsl_ops=DSL_OPS, device=device, cfg=alpha_cfg, \n",
    "                      policy_net=policy_net, value_net=value_net)\n",
    "dsl_shim = _DSLShim(model)\n",
    "\n",
    "# Near-miss repair settings\n",
    "NEAR_MISS_THRESHOLD = 0.70\n",
    "\n",
    "for idx in tqdm(range(len(dataset)), desc='Evaluating'):\n",
    "    # Reset counter per task (critical for avoiding recursion errors!)\n",
    "    if hasattr(model, '_forward_call_count'):\n",
    "        model._forward_call_count = {}\n",
    "    \n",
    "    try:\n",
    "        demos, test_inputs, test_outputs, task_id = dataset[idx]\n",
    "        task_predictions = []\n",
    "        \n",
    "        for test_input in test_inputs:\n",
    "            # Convert to list format for AlphaEvolve\n",
    "            test_list = test_input.cpu().tolist() if isinstance(test_input, torch.Tensor) else test_input\n",
    "            demos_list = [(d[0].cpu().tolist() if isinstance(d[0], torch.Tensor) else d[0],\n",
    "                          d[1].cpu().tolist() if isinstance(d[1], torch.Tensor) else d[1])\n",
    "                         for d in demos]\n",
    "            \n",
    "            # Run AlphaEvolve (returns 2 attempts)\n",
    "            attempts_list = evolver.solve_task(demos_list, test_list)\n",
    "            pred_attempts = [torch.tensor(a, device=device) for a in attempts_list]\n",
    "            \n",
    "            # Apply near-miss repair to both attempts if ground truth available\n",
    "            if idx < len(test_outputs):\n",
    "                target = test_outputs[idx] if idx < len(test_outputs) else None\n",
    "                if target is not None:\n",
    "                    if isinstance(target, list):\n",
    "                        target = torch.tensor(target, device=device)\n",
    "                    else:\n",
    "                        target = target.to(device)\n",
    "                    \n",
    "                    for att_idx in range(len(pred_attempts)):\n",
    "                        pred_tensor = pred_attempts[att_idx].squeeze()\n",
    "                        target_sq = target.squeeze()\n",
    "                        \n",
    "                        if pred_tensor.shape == target_sq.shape:\n",
    "                            acc = (pred_tensor == target_sq).float().mean().item()\n",
    "                            \n",
    "                            if acc >= NEAR_MISS_THRESHOLD and acc < 1.0:\n",
    "                                repaired, ops, improvement, _ = near_miss_repair(\n",
    "                                    pred_tensor, target_sq,\n",
    "                                    dsl_ops=DSL_OPS,\n",
    "                                    dsl_shim=dsl_shim,\n",
    "                                    max_repairs=2,\n",
    "                                    distance_threshold=int(pred_tensor.numel() * (1.0 - NEAR_MISS_THRESHOLD)),\n",
    "                                    similarity_threshold=NEAR_MISS_THRESHOLD\n",
    "                                )\n",
    "                                \n",
    "                                if improvement > 0:\n",
    "                                    pred_attempts[att_idx] = repaired\n",
    "            \n",
    "            # Convert to list format and add to predictions\n",
    "            for pred in pred_attempts:\n",
    "                pred_list = pred.cpu().tolist() if isinstance(pred, torch.Tensor) else pred\n",
    "                task_predictions.append(pred_list)\n",
    "        \n",
    "        submission[task_id] = task_predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Task {idx} ({task_id}) failed: {e}')\n",
    "        # Add fallback empty predictions\n",
    "        submission[task_id] = [[[0]], [[0]]]\n",
    "\n",
    "print(f'\\n‚úÖ Evaluation complete! {len(submission)} tasks processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Submission\n",
    "output_path = '/kaggle/working/submission.json'\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(submission, f)\n",
    "\n",
    "print(f'‚úÖ Submission saved: {output_path}')\n",
    "print(f'‚úÖ File size: {os.path.getsize(output_path) / 1024:.1f} KB')\n",
    "print(f'‚úÖ Tasks in submission: {len(submission)}')\n",
    "\n",
    "# Validate format\n",
    "sample_task = list(submission.keys())[0]\n",
    "sample_preds = submission[sample_task]\n",
    "print(f'\\nSample task {sample_task}: {len(sample_preds)} attempts')\n",
    "print('‚úÖ Ready for Kaggle submission!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
